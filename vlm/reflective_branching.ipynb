{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETOT VLM Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm import BaseVLM, MultiVLM\n",
    "api_key = \"\"\n",
    "base_url = \"\"\n",
    "model = \"gpt-4o\"\n",
    "erp = \"\"\n",
    "system_message = \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Failure Detection\n",
    "\n",
    "Analyze the changes in the state of each object through the first and last frames of the action. \n",
    "\n",
    "And predict the ideal state of the objects in the future based on the subtask.\n",
    "\n",
    "If it is found that the change in the state of the object does not meet expectations, the action is considered a failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOAL State:\n",
      "1. [open, microwave]\n",
      "2. [ontop, microwave, desk]\n",
      "3. [ontop, tennis, desk]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "task = \"[OPEN, microwave]\"\n",
    "env_objects = \"microwave, tennis, desk\"\n",
    "object_state = \"1. [closed, microwave] 2. [ontop, microwave, desk] 3. [ontop, tennis, desk]\"\n",
    "\n",
    "content = '''\n",
    "Task background: The robot is in front of the desk and needs to complete a multi-step task.\n",
    "You need to generate some criteria for determining whether the task has been successfully executed, which will be handed over to the simulator for evaluation. \n",
    "You can make requirements for the position relationship of objects (up and down), the orientation of objects (vertical up), the state of the articulated object (open/closed)and so on.\n",
    "\n",
    "Example1:\n",
    "Input:\n",
    "Task: [PICK UP, apple 1]\n",
    "Environment_object: apple 1, apple 2, cabinet, plate, desk\n",
    "Object_state:\n",
    "1. [ontop, apple 1, desk]\n",
    "2. [ontop, apple 2, desk]\n",
    "3. [ontop, cabinet, desk]\n",
    "4. [closed, cabinet]\n",
    "5. [ontop, plate, desk]\n",
    "Output:\n",
    "GOAL State:\n",
    "1. [inhand, apple 1]\n",
    "2. [ontop, apple 2, plate]\n",
    "3. [ontop, cabinet, plate]\n",
    "4. [closed, cabinet]\n",
    "5. [ontop, plate, desk]\n",
    "\n",
    "Example2:\n",
    "Input:\n",
    "Task: [PUT INTO, holder]\n",
    "Environment_object: tennis, holder, desk\n",
    "Object_state:\n",
    "1. [inhand, tennis]\n",
    "2. [ontop, holder, desk]\n",
    "Output:\n",
    "GOAL State:\n",
    "1. [inside, tennis, holder]\n",
    "2. [ontop, holder, desk]\n",
    "\n",
    "Example2:\n",
    "Input:\n",
    "Task: [PICK UP, tennis]\n",
    "Environment_object: tennis, holder, desk\n",
    "Object_state:\n",
    "1. [ontop, tennis, desk]\n",
    "2. [ontop, holder, desk]\n",
    "Output:\n",
    "GOAL State:\n",
    "1. [inhand, tennis]\n",
    "2. [ontop, holder, desk]\n",
    "\n",
    "Input:\n",
    "Task: {task}\n",
    "Environment_object: {env_objects}\n",
    "Object_state:\n",
    "{object_state}\n",
    "Output:\n",
    "\n",
    "'''\n",
    "content1 = content.format(task=task, env_objects=env_objects, object_state=object_state)\n",
    "# 使用MultiVLM\n",
    "base_vlm = BaseVLM(api_key, model, system_message, base_url)\n",
    "response = base_vlm.call_model(content1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOAL State:\n",
      "1. [inside, apple, drawer]\n",
      "2. [ontop, drawer, desk]\n",
      "3. [open, drawer]\n"
     ]
    }
   ],
   "source": [
    "task = \"[PUT INTO, drawer]\"\n",
    "env_objects = \"apple, drawer, desk\"\n",
    "object_state = \"1. [open, drawer] 2. [ontop, drawer, desk] 3. [inhand, apple]\"\n",
    "content2 = content.format(task=task, env_objects=env_objects, object_state=object_state)\n",
    "base_vlm = BaseVLM(api_key, model, system_message, base_url)\n",
    "response = base_vlm.call_model(content2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from vlm import BaseVLM, MultiVLM, HistoryVLM\n",
    "cap = cv2.VideoCapture('./examples/videos/1_s_f.mp4')\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "key_frames = 10\n",
    "interval = total_frames // key_frames\n",
    "\n",
    "for i in range(key_frames):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - (i) * interval - 1)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(f'./examples/images/vlm_frames/frame_{key_frames-1-i}.jpg', frame)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLM Response: Before action:  \n",
      "- Microwave is closed  \n",
      "- Tennis ball is on the desk  \n",
      "\n",
      "After action:  \n",
      "- Microwave is open  \n",
      "- Tennis ball is unsure (possibility of being inside the microwave or lost due to visual obstruction)\n",
      "\n",
      "The exact state of the tennis ball after the action is unclear because the microwave door might be obstructing the view.\n"
     ]
    }
   ],
   "source": [
    "task = \"open the microwave\"\n",
    "env_objects = \"microwave, tennis, desk\"\n",
    "action = \"OPEN, microwave\"\n",
    "image_url = ['./examples/images/vlm_frames/frame_0.jpg', './examples/images/vlm_frames/frame_9.jpg']\n",
    "content = '''\n",
    "Environmental objects include: {env_objects}.\n",
    "Action: {action}\n",
    "Check step by step the states of environmental objects before and after the action.\n",
    "For example: \n",
    "Before action: box is on the desk; apple is in the holder, tennis is in robot gripper.\n",
    "After action: box is on the desk; apple is in the holder, tennis is on the desk.\n",
    "If the object is not in the scene, then it is lost.\n",
    "If the action being performed is to close or open a articulated object, the loss of the object may be due to visual obstruction, and you need to reason carefully.If you are unsure, output 'unsure'\n",
    "'''\n",
    "content = content.format(env_objects=env_objects, action=action)\n",
    "multi_vlm = MultiVLM(api_key, model, system_message, base_url, erp=erp)\n",
    "multi_response0 = multi_vlm.call_model_with_multi(content, image_url)\n",
    "print(\"VLM Response:\", multi_response0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For objects classified as unsure, we further verify their states in simulation using their precise pose information. \n",
    "\n",
    "During real execution, if an object is judged as unsure, we compare it with its corresponding case in the simulation plan: if the object likewise appeared unsure after executing the same action in simulation, we update the object's state to the verified state determined in simulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reflective Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = '''\n",
    "Task background: The robot is in front of the desk and needs to complete a multi-step task.\n",
    "Now, it has encountered a failure during subtask execution.\n",
    "\n",
    "The subtask is {subtask}. The complete task plan is {task}. Environmental objects include: {env_objects}.\n",
    "The two images show the initial state and the end state of the environment.\n",
    "Some objects' state changes do not meet expectations: before task execution, {object_origin_state}, but after execution, it should be {object_target_state} but did not meet the requirements, combining images to analyze what causes it.\n",
    "\n",
    "For example: \n",
    "Opening the drawer knocked over the vase;\n",
    "Picking up an apple knocked over the pen holder next to the apple.\n",
    "\n",
    "Action Skills Set:\n",
    "- PICK UP\n",
    "- PUT ON\n",
    "- PUT INTO\n",
    "- OPEN\n",
    "- CLOSE\n",
    "Consider whether it is possible to correct the task using only the action skills set, and be careful not to change the object without authorization. \n",
    "For example, correcting a failed capture of tennis 1 should not be about grabbing tennis 2, but about how to improve the planning for tennis 1.\n",
    "Note that only actions from the action skills can be used to form a plan. If there is no possible correction solution, output 'no'!!!\n",
    "Do not change the purpose of the object's operation, for example, if the original plan intended to put the apple in the holder, the new plan should maintain it.\n",
    "\n",
    "###You only have three abilities for correction: \n",
    "### 1. If an object is knocked over outside the desktop due to your actions, move the object to a safe place first.\n",
    "Example 1:\n",
    "The task is open the drawer.\n",
    "The origin task plan is 1. [OPEN, drawer]\n",
    "I observed that opening the drawer would knock over the book, so please move the book away before opening the drawer\n",
    "Correct task: \n",
    "1. [PICK UP, book] \n",
    "2. [PUT ON, desk](safe place) \n",
    "3. [OPEN, drawer]\n",
    "\n",
    "### 2. If the action of picking up a container fails due to the container containing other objects, you should first pick up and put the other objects in a safe place, and then pick up the container \n",
    "Example 2:\n",
    "The task is put the tennis into the holder and put them on the drawer.\n",
    "The origin task plan is 1. [PICK UP, tennis] 2. [PUT INTO, holder] 3. [PICK UP, holder] 4. [PUT ON, drawer]\n",
    "I noticed that loading tennis into the holder first would make it impossible to grab the holder, so I need to first grab the holder and place it in the designated location before loading tennis into the holder.\n",
    "Correct task: \n",
    "1. [PICK UP, holder] \n",
    "2. [PUT ON, desk] \n",
    "3. [PICK UP, tennis] \n",
    "4. [PUT INTO, holder]\n",
    "\n",
    "### 3. If the opening/closing of an articulated object causes other objects to be obstructed and unable to operate, you should move the target object to a safe position before opening/closing the articulated object.\n",
    "Example 3:\n",
    "The task is pick up the eraser and put it into the drawer.\n",
    "The origin task plan is 1. [OPEN, drawer] 2. [PICK UP, eraser] 3. [PUT INTO, drawer] 4. [CLOSE, drawer]\n",
    "I observed that the part of the opening drawer obstruct the view and path of the eraser. The first step [OPEN, drawer] cause the eraser to be obstructed, so please move the eraser away before opening the drawer\n",
    "Correct task: \n",
    "3. [PICK UP, eraser](In order not to be obstructed after opening the drawer) \n",
    "4. [PUT ON, drawer(safe place)](Make sure there are no objects in hand)\n",
    "5. [OPEN, drawer](Then, open and put it in)\n",
    "6. [PICK UP, eraser] \n",
    "7. [PUT INTO, drawer] \n",
    "8. [CLOSE, drawer]\n",
    "\n",
    "### If it is due to a mismatch in the physical properties of the object itself, such as an object that is too large to fit into a drawer, an object that is too heavy causing the container to fall, etc., you do not need to correct the plan!!!\n",
    "### If the object causing the failure is one of the two objects involved in the PUT IN or PUT ON action, it is considered an unchangeable problem of the task itself, and the output is None!!!\n",
    "Example 4:\n",
    "The task is put the corn into the bowl.\n",
    "The origin task plan is 1. [PUT INTO, bowl]\n",
    "Because the size of the corn itself does not match the bowl, it causes the bowl to tilt, so there is no improvement plan.\n",
    "Correct task: None\n",
    "\n",
    "The task is {task}\n",
    "The origin task plan is {task_plan}.\n",
    "I observed that:\n",
    "'''\n",
    "\n",
    "screen = '''\n",
    "Task background: The robot is in front of the desk and needs to complete a multi-step task.\n",
    "Given a corrective plan for the observed failure, determine whether this plan relies on reinforcing or stabilizing an object (e.g., holding, supporting, wedging, or otherwise increasing its stability).\n",
    "On the contrary, methods such as removing obstacles and adjusting the order of movements are not simply reinforcing ideas, but are acceptable.\n",
    "If the corrective plan involves reinforcing/stabilizing an object, answer: Yes, else answer: No.\n",
    "Corrective plan:{plan}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_check = '''\n",
    "Task background: The robot is in front of the desk and needs to complete a multi-step task.\n",
    "You need to check if there are any irregularities in the generated tasks. The task specifications are as follows:\n",
    "1. Firstly, the plan must consist of the following actions, and any undefined actions will be considered as errors:\n",
    "Action Skills Set:\n",
    "- PICK UP\n",
    "- PUT ON\n",
    "- PUT INTO\n",
    "- OPEN\n",
    "- CLOSE\n",
    "\n",
    "2. We require that when multiple objects are placed into a articulated container, each insertion must follow an open–pick-putin–close cycle. That is, the container must be opened, the object placed inside, and then closed before inserting the next object.\n",
    "\n",
    "3. After action [PICK UP, object], the next action can only be PUT ON, PUT INTO, or None(PICK UP is the last action), not OPEN or CLOSE. While after action [PUT ON, obj] or [PUT INTO, obj] or [CLOSE, obj] or [OPEN, obj], the next action can be any action.\n",
    "\n",
    "4. Microwave ovens and drawers are articulated objects, holders are not articulated objects, but they can all be put into objects as containers. Drawer can also be used as a platform to place objects.\n",
    "\n",
    "5. Plans for different branches should be judged separately, There is no relationship between different branches!!!\n",
    "\n",
    "6. Output format: First output Yes!!!/No!!!, then output analysis.\n",
    "\n",
    "7. The PICK UP, OPEN, and CLOSE actions can appear alone in the plan.\n",
    "\n",
    "8. After PICK UP, you can only PICK UP again after PUT ON, PUT INTO.\n",
    "\n",
    "Example1:\n",
    "Input:\n",
    "1. [OPEN, drawer]\n",
    "Output:\n",
    "Yes, The final generated plan meets the specifications, OPEN action can be excuted alone\n",
    "\n",
    "Example2:\n",
    "Input:\n",
    "1. [CLOSE, drawer]\n",
    "Output:\n",
    "Yes, The final generated plan meets the specifications, CLOSE action can be excuted alone\n",
    "\n",
    "Example3:\n",
    "Input:\n",
    "1. [PICK UP, apple]\n",
    "2. [PUT ON, drawer] (safe place)\n",
    "Output:\n",
    "Yes!!! The final generated plan meets the specifications. \n",
    "The first action is picking up the apple. After [PICK UP apple], you can do second action [PUT ON, drawer].\n",
    "\n",
    "Example4:\n",
    "Input:\n",
    "Branch 1:\n",
    "1. [PICK UP, apple 1]\n",
    "Branch 2:\n",
    "1. [PICK UP, apple 2]\n",
    "Output:\n",
    "Yes!!! Plans for different branches should be judged separately. Each branch plan meets the specifications.\n",
    "PICK UP action can be excuted alone for each branch, actions. \n",
    "After picking up an object, the next action can be None(PICK UP is the last action)\n",
    "\n",
    "Example5:\n",
    "Input:\n",
    "Branch 1:\n",
    "1. [PICK UP, tennis]\n",
    "2. [PUT INTO, holder]\n",
    "3. [OPEN, drawer]\n",
    "4. [PICK UP, apple]\n",
    "5. [PUT INTO, drawer]\n",
    "6. [CLOSE, drawer]\n",
    "Output:\n",
    "Yes!!! The final generated plan meets the specifications.\n",
    "The first action is picking up the tennis, the second action can be [PUT INTO, holder]. \n",
    "And after [PUT INTO, holder], the third action can be [PICK UP, apple].\n",
    "And after [OPEN, drawer], the fourth action can be [PICK UP, apple].\n",
    "And after [OPEN, drawer] and [PICK UP, apple], the sixth action can be [PUT INTO, drawer].\n",
    "And after [PUT INTO, drawer], the fifth action can be [CLOSE, drawer].\n",
    "\n",
    "\n",
    "\n",
    "Example6:\n",
    "Input:\n",
    "MultiVLM Response: In analyzing the images and the scenario, we can infer the issue might have occurred while attempting to pick up the tennis ball after opening the drawer. This action was likely impeded by the drawer being open, hindering the robot's access to the tennis ball on the desk. \n",
    "\n",
    "To correct the task plan while adhering to only the action skills set provided, we can modify the steps to move the tennis ball to a safe place before dealing with the drawer.\n",
    "\n",
    "Here is the corrected task plan:\n",
    "\n",
    "1. [PICK UP, apple]\n",
    "2. [PUT INTO, holder]\n",
    "3. [PICK UP, tennis]\n",
    "4. [OPEN, drawer]\n",
    "5. [PUT INTO, drawer]\n",
    "6. [CLOSE, drawer]\n",
    "\n",
    "Output:\n",
    "No!!! The final generated plan encountered an error: the forth action OPEN drawer cannot be executed directly after the third action picking up tennis.\n",
    "\n",
    "Example7:\n",
    "Input:\n",
    "MultiVLM Response: \n",
    "Correct task:\n",
    "1. [OPEN, holder]\n",
    "2. [PICK UP, apple]\n",
    "3. [PUT INTO, holder]\n",
    "4. [CLOSE, holder]\n",
    "Output:\n",
    "No!!! The holder is not a articulated object and cannot be opened\n",
    "\n",
    "Example8:\n",
    "Input:\n",
    "Branch 1:\n",
    "1. [PICK UP, apple 1]\n",
    "1. [PICK UP, apple 2]\n",
    "Output:\n",
    "No!!! The final generated plan encountered an error: the second action pick up apple 2 cannot be executed directly after the first action picking up apple 1\n",
    "\n",
    "Example9:\n",
    "Input:\n",
    "MultiVLM Response: \n",
    "Correction:\n",
    "1. [PICK UP, tennis 1] (move to accessible position)\n",
    "2. [PUT ON, desk](safe and accessible place)\n",
    "3. [PICK UP, tennis 2] \n",
    "4. [PUT INTO, holder]\n",
    "5. [PICK UP, holder]\n",
    "6. [PUT ON, drawer](safe and accessible place)\n",
    "Output:\n",
    "Yes!!! The final generated plan meets the specifications. \n",
    "After the first action picking up the tennis 1, the second action can be [PUT ON, desk]. \n",
    "And after the second action [PUT ON, desk], the third action can be [PICK UP, tennis 2]. \n",
    "After the third action picking up the tennis 2, the fourth action can be [PUT INTO, holder]. \n",
    "And after the fourth action [PUT INTO, holder], the fifth action can be [PICK UP, holder]. \n",
    "And after the fifth action [PICK UP, holder], the sixth action can be [PUT ON, drawer].\n",
    "\n",
    "\n",
    "You only need to focus on the corrected plan and don't need to think about the specific purpose of the plan. You just need to strictly check whether each action meets the specifications\n",
    "\n",
    "### Input:\n",
    "{task_plan}\n",
    "\n",
    "Output:\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflective_plan(task, task_plan, subtask, env_objects, object_origin_state, object_target_state, image_url):\n",
    "\n",
    "    content1 = content.format(task=task, task_plan=task_plan, subtask=subtask, env_objects=env_objects, object_origin_state=object_origin_state, object_target_state=object_target_state)\n",
    "    multi_vlm = MultiVLM(api_key, model, system_message, base_url, erp)\n",
    "    format = \"NO\"\n",
    "    while \"yes\" not in format.lower():\n",
    "        plan = multi_vlm.call_model_with_multi(content1, image_url)\n",
    "        print(\"PLAN!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(\"MultiVLM Response:\", plan)\n",
    "        print(\"FORMAT!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        if 'none' in plan.lower():\n",
    "            plan = \"NONE\"\n",
    "            print(\"NONE!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            return plan\n",
    "        format_prompt = format_check.format(task_plan=plan)\n",
    "        format = base_vlm.call_model(format_prompt)\n",
    "        print(format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from vlm import BaseVLM, MultiVLM, HistoryVLM\n",
    "cap = cv2.VideoCapture('./examples/videos/1_s_f.mp4')\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "key_frames = 10\n",
    "interval = total_frames // key_frames\n",
    "\n",
    "for i in range(key_frames):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - (i) * interval - 1)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(f'./examples/images/vlm_frames/frame_{key_frames-1-i}.jpg', frame)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLAN!!!!!!!!!!!!!!!!!!!!!!\n",
      "MultiVLM Response: When opening the microwave door, it appears that the tennis ball, which was initially on top of the desk, got knocked off its position. Here is the correction plan using the provided action skills:\n",
      "\n",
      "Corrected Task Plan:\n",
      "1. [PICK UP, tennis]\n",
      "2. [PUT ON, desk] (move the tennis ball to a safe place on the desk, away from where the microwave door will swing)\n",
      "3. [OPEN, microwave]\n",
      "\n",
      "This plan ensures that opening the microwave door does not knock over or displace the tennis ball.\n",
      "FORMAT!!!!!!!!!!!!!!!!!!!!!!\n",
      "Yes!!! The final generated plan meets the specifications.\n",
      "\n",
      "After the first action picking up the tennis, the second action can be [PUT ON, desk]. And after the second action [PUT ON, desk], the third action can be [OPEN, microwave].\n"
     ]
    }
   ],
   "source": [
    "\n",
    "task = \"Open the microwave door\"\n",
    "task_plan = \"1. [OPEN, microwave]\"\n",
    "subtask = \"[OPEN, microwave]\"\n",
    "env_objects = \"microwave, tennis, desk\"\n",
    "object_origin_state = \"[ontop, tennis, desk]\"\n",
    "object_target_state = \"[ontop, tennis, desk]\"\n",
    "image_url = ['./examples/images/vlm_frames/frame_0.jpg', './examples/images/vlm_frames/frame_2.jpg', './examples/images/vlm_frames/frame_4.jpg']\n",
    "\n",
    "reflective_plan(task=task, task_plan=task_plan, subtask=subtask, env_objects=env_objects, object_origin_state=object_origin_state, object_target_state=object_target_state, image_url=image_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from vlm import BaseVLM, MultiVLM, HistoryVLM\n",
    "cap = cv2.VideoCapture('./examples/videos/2_f.mp4')\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "key_frames = 10\n",
    "interval = total_frames // key_frames\n",
    "\n",
    "for i in range(key_frames):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - (i) * interval - 1)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(f'./examples/images/vlm_frames/frame_{key_frames-1-i}.jpg', frame)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLAN!!!!!!!!!!!!!!!!!!!!!!\n",
      "MultiVLM Response: I observed that the 'put into' action was not successful. Based on the images, the pen appears to be misaligned with the holder and is resting outside the holder.\n",
      "\n",
      "To correct this issue, I would suggest a modified plan that involves reorienting the pen before attempting to place it in the holder. However, given that the failure might also be due to a mismatch in the object's properties or the robot's misalignment during the execution, it seems the objects involved in the \"PUT INTO\" action are the source of the problem. Since this issue pertains to the placement of the pen into the holder, which directly involves the two objects in the task, it is considered an unchangeable problem of the task itself.\n",
      "\n",
      "Therefore, the correct task adjustment plan for this particular scenario is:\n",
      "\n",
      "None.\n",
      "FORMAT!!!!!!!!!!!!!!!!!!!!!!\n",
      "NONE!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NONE'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "task = \"Reorient the pen and place it into a pen holder\"\n",
    "task_plan = \"1. [PICK UP, pen] 2. [PUT INTO, holder 1]\"\n",
    "subtask = \"[PUT INTO, holder 1]\"\n",
    "env_objects = \"pen, apple, drawer, holder 1, holder 2, desk\"\n",
    "object_origin_state = \"[inhand, pen]\"\n",
    "object_target_state = \"[inside, pen, holder 1]\"\n",
    "image_url = ['./examples/images/vlm_frames/frame_7.jpg', './examples/images/vlm_frames/frame_8.jpg', './examples/images/vlm_frames/frame_9.jpg']\n",
    "\n",
    "reflective_plan(task=task, task_plan=task_plan, subtask=subtask, env_objects=env_objects, object_origin_state=object_origin_state, object_target_state=object_target_state, image_url=image_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from vlm import BaseVLM, MultiVLM, HistoryVLM\n",
    "cap = cv2.VideoCapture('./examples/videos/4_s_f.mp4')\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "key_frames = 10\n",
    "interval = total_frames // key_frames\n",
    "\n",
    "for i in range(key_frames):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - (i) * interval - 1)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(f'./examples/images/vlm_frames/frame_{key_frames-1-i}.jpg', frame)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLAN!!!!!!!!!!!!!!!!!!!!!!\n",
      "MultiVLM Response: I observed that the drawer cannot be closed because the toy inside the drawer is obstructing it from fully closing. To correct this, the toy must be removed from the drawer before attempting to close it. The new task plan is:\n",
      "\n",
      "1. [PICK UP, toy]\n",
      "2. [PUT ON, desk] (safe place)\n",
      "3. [CLOSE, drawer]\n",
      "FORMAT!!!!!!!!!!!!!!!!!!!!!!\n",
      "No!!! The final generated plan encountered an error: the third action CLOSE drawer cannot be executed directly after the second action PUT ON desk.\n",
      "\n",
      "Explanation:\n",
      "According to the specifications, after a PUT ON action, the next action can be any action. However, in this specific context, to close the drawer, it should first be opened (or already be open). The plan does not include an action to open the drawer before attempting to close it.\n",
      "\n",
      "Here’s how you could correct the plan to meet the specifications:\n",
      "\n",
      "1. [OPEN, drawer]\n",
      "2. [PICK UP, toy]\n",
      "3. [PUT ON, desk] (safe place)\n",
      "4. [CLOSE, drawer]\n",
      "PLAN!!!!!!!!!!!!!!!!!!!!!!\n",
      "MultiVLM Response: The task is to close the drawer. The original task plan is 1. [CLOSE, drawer].\n",
      "\n",
      "I observed that the toy inside the drawer is obstructing its closure. \n",
      "\n",
      "The correct task plan should be:\n",
      "1. [PICK UP, toy] (to clear the obstruction)\n",
      "2. [PUT ON, desk] (place the toy in a safe place)\n",
      "3. [CLOSE, drawer]\n",
      "FORMAT!!!!!!!!!!!!!!!!!!!!!!\n",
      "### Input:\n",
      "The task is to close the drawer. The original task plan is 1. [CLOSE, drawer].\n",
      "\n",
      "I observed that the toy inside the drawer is obstructing its closure. \n",
      "\n",
      "The correct task plan should be:\n",
      "1. [PICK UP, toy] (to clear the obstruction)\n",
      "2. [PUT ON, desk] (place the toy in a safe place)\n",
      "3. [CLOSE, drawer]\n",
      "\n",
      "Output:\n",
      "Yes!!! The final generated plan meets the specifications.\n",
      "After the first action picking up the toy, the second action can be [PUT ON, desk]. \n",
      "And after the second action [PUT ON, desk], the third action can be [CLOSE, drawer].\n"
     ]
    }
   ],
   "source": [
    "task = \"Close the drawer\"\n",
    "task_plan = \"1. [CLOSE, drawer]\"\n",
    "subtask = \"[CLOSE, drawer]\"\n",
    "env_objects = \"drawer, toy, desk\"\n",
    "object_origin_state = \"[open, drawer]\"\n",
    "object_target_state = \"[closed, drawer]\"\n",
    "image_url = ['./examples/images/vlm_frames/frame_0.jpg', './examples/images/vlm_frames/frame_5.jpg', './examples/images/vlm_frames/frame_9.jpg']\n",
    "\n",
    "reflective_plan(task, task_plan, subtask, env_objects, object_origin_state, object_target_state, image_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from vlm import BaseVLM, MultiVLM, HistoryVLM\n",
    "cap = cv2.VideoCapture('./examples/videos/5_s_f.mp4')\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "key_frames = 10\n",
    "interval = total_frames // key_frames\n",
    "\n",
    "for i in range(key_frames):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - (i) * interval - 1)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(f'./examples/images/vlm_frames/frame_{key_frames-1-i}.jpg', frame)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLAN!!!!!!!!!!!!!!!!!!!!!!\n",
      "MultiVLM Response: the pen was not placed into holder 2 as required. Upon examining the images, the issue seems to arise from the presence of the apple on top of holder 2, which obstructs the pen from being placed properly into the holder.\n",
      "\n",
      "To correct this, we need to move the apple to a safe place before attempting to place the pen into holder 2. Here is the corrected task plan:\n",
      "\n",
      "1. [PICK UP, apple] \n",
      "2. [PUT ON, desk] \n",
      "3. [PICK UP, pen] \n",
      "4. [PUT INTO, holder 2]\n",
      "\n",
      "By following this corrected task plan, we can ensure that the pen is successfully placed into holder 2 without any obstruction.\n",
      "FORMAT!!!!!!!!!!!!!!!!!!!!!!\n",
      "No!!! The final generated plan encountered an error: \n",
      "\n",
      "Analysis:\n",
      "- The first action is picking up the apple, which is valid.\n",
      "- The second action is putting the apple on the desk, which is valid.\n",
      "- The third action is picking up the pen, which is valid.\n",
      "\n",
      "The fourth action, [PUT INTO, holder 2], is also valid after the third action. \n",
      "\n",
      "However, the plan meets all the specified rules of action sequence and does not contain errors that violate the specifications. Therefore, the output should actually be:\n",
      "\n",
      "Yes!!! The final generated plan meets the specifications.\n",
      "The first action is picking up the apple, following correctly with the second action [PUT ON, desk]. After putting the apple on the desk, the plan correctly follows with picking up the pen and putting it into holder 2 without any violations.\n"
     ]
    }
   ],
   "source": [
    "task = \"Reorient the pen and place it into a pen holder\"\n",
    "task_plan = \"1. [PICK UP, pen] 2. [PUT INTO, holder 2]\"\n",
    "subtask = \"[PUT INTO, holder 2]\"\n",
    "env_objects = \"pen, apple, drawer, holder 1, holder 2, desk\"\n",
    "object_origin_state = \"[inhand, pen]\"\n",
    "object_target_state = \"[inside, pen, holder 2]\"\n",
    "image_url = ['./examples/images/vlm_frames/frame_7.jpg', './examples/images/vlm_frames/frame_8.jpg', './examples/images/vlm_frames/frame_9.jpg']\n",
    "\n",
    "reflective_plan(task=task, task_plan=task_plan, subtask=subtask, env_objects=env_objects, object_origin_state=object_origin_state, object_target_state=object_target_state, image_url=image_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "task = \"[OPEN, microwave]\"\n",
    "env_objects = \"microwave, tennis, desk\"\n",
    "object_state = \"1. [closed, microwave] 2. [ontop, microwave, desk] 3. [ontop, tennis, desk]\"\n",
    "plan = '''1. [PICK UP, tennis] (Remove the tennis ball from the area that will be obstructed.)\n",
    "2. [PUT ON, a different part of the desk (safe place)] (Place the tennis ball in a safe location on the desk where it will not fall due to the door opening.)\n",
    "3. [OPEN, microwave] (Proceed with opening the microwave door.)'''\n",
    "screen1 = screen.format(plan=plan)\n",
    "# 使用MultiVLM\n",
    "base_vlm = BaseVLM(api_key, model, system_message, base_url)\n",
    "response = base_vlm.call_model(screen1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from vlm import BaseVLM, MultiVLM, HistoryVLM\n",
    "cap = cv2.VideoCapture('./examples/videos/6_s_f.mp4')\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "key_frames = 10\n",
    "interval = total_frames // key_frames\n",
    "\n",
    "for i in range(key_frames):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - (i) * interval - 1)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(f'./examples/images/vlm_frames/frame_{key_frames-1-i}.jpg', frame)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLAN!!!!!!!!!!!!!!!!!!!!!!\n",
      "MultiVLM Response: I observed that the action of picking up the holder failed because the apple was placed inside the holder, which made it difficult to grasp the holder. \n",
      "\n",
      "To correct this task, I need to pick up and place the holder in the designated location first before placing the apple inside the holder.\n",
      "\n",
      "Correct task plan:\n",
      "1. [PICK UP, holder] \n",
      "2. [PUT ON, drawer]\n",
      "3. [PICK UP, apple] \n",
      "4. [PUT INTO, apple, holder]\n",
      "FORMAT!!!!!!!!!!!!!!!!!!!!!!\n",
      "No!!! The final generated plan encountered an error: the fourth action [PUT INTO, apple, holder] is incorrectly formatted and should only specify the object being placed and the container, i.e., [PUT INTO, holder]. Additionally, there seems to be an error in action sequence as [PUT INTO] should directly follow the [PICK UP] action of the object being placed into the container. Below is the corrected task plan:\n",
      "\n",
      "1. [PICK UP, holder] \n",
      "2. [PUT ON, drawer]\n",
      "3. [PICK UP, apple] \n",
      "4. [PUT INTO, holder]\n",
      "\n",
      "This plan now follows the defined action set skills and the permissible sequence of actions according to the specifications. \n",
      "\n",
      "Output: \n",
      "Yes!!! The final generated plan meets the specifications. \n",
      "The first action is picking up the holder, the second action can be [PUT ON, drawer]. \n",
      "After the second action [PUT ON, drawer], the third action can be [PICK UP, apple]. \n",
      "And after the third action [PICK UP, apple], the fourth action can be [PUT INTO, holder].\n"
     ]
    }
   ],
   "source": [
    "task = \"Place the apple and the holder on the drawer, with the apple inside the holder\"\n",
    "task_plan = \"1. [PICK UP, apple] 2. [PUT INTO, apple, holder] 3. [PICK UP, holder] 4. [PUT ON, holder, drawer]\"\n",
    "subtask = \"[PICK UP, holder]\"\n",
    "env_objects = \"apple, drawer, holder, desk\"\n",
    "object_origin_state = \"[ontop, holder, desk]\"\n",
    "object_target_state = \"[inhand, holder]\"\n",
    "image_url = ['./examples/images/vlm_frames/frame_7.jpg', './examples/images/vlm_frames/frame_8.jpg', './examples/images/vlm_frames/frame_9.jpg']\n",
    "\n",
    "reflective_plan(task=task, task_plan=task_plan, subtask=subtask, env_objects=env_objects, object_origin_state=object_origin_state, object_target_state=object_target_state, image_url=image_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from vlm import BaseVLM, MultiVLM, HistoryVLM\n",
    "cap = cv2.VideoCapture('./examples/videos/7_s_f.mp4')\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "key_frames = 10\n",
    "interval = total_frames // key_frames\n",
    "\n",
    "for i in range(key_frames):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - (i) * interval - 1)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(f'./examples/images/vlm_frames/frame_{key_frames-1-i}.jpg', frame)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLAN!!!!!!!!!!!!!!!!!!!!!!\n",
      "MultiVLM Response: the execution of step 4 [PICK UP, tennis] failed due to the tennis ball being blocked by the open drawer. To correct this, we need to adjust the task plan to avoid this obstruction.\n",
      "\n",
      "Here's the corrected task plan:\n",
      "\n",
      "1. [PICK UP, apple] \n",
      "2. [PUT INTO, holder]\n",
      "3. [PICK UP, tennis]  (Ensure this action is not obstructed)\n",
      "4. [OPEN, drawer]\n",
      "5. [PUT INTO, drawer]\n",
      "6. [CLOSE, drawer] \n",
      "\n",
      "By picking up the tennis ball before opening the drawer, we avoid the failure caused by the open drawer blocking access to the tennis ball.\n",
      "FORMAT!!!!!!!!!!!!!!!!!!!!!!\n",
      "No!!! The final generated plan encountered an error: the fourth action OPEN drawer cannot be executed directly after the third action picking up tennis.\n",
      "\n",
      "Analysis:\n",
      "1. [PICK UP, apple] - Valid\n",
      "2. [PUT INTO, holder] - Valid\n",
      "3. [PICK UP, tennis] - Valid\n",
      "4. [OPEN, drawer] - Invalid (as the next action after PICK UP can only be PUT ON, PUT INTO, or None, not OPEN)\n",
      "5. [PUT INTO, drawer] - Conditional Validity (pending prior steps)\n",
      "6. [CLOSE, drawer] - Conditional Validity (pending prior steps)\n",
      "\n",
      "To correct the task plan, the action following a PICK UP should adhere to the allowed actions. Here’s a revised version:\n",
      "\n",
      "Corrected task plan:\n",
      "1. [PICK UP, apple] \n",
      "2. [PUT INTO, holder]\n",
      "3. [OPEN, drawer]\n",
      "4. [PICK UP, tennis]\n",
      "5. [PUT INTO, drawer]\n",
      "6. [CLOSE, drawer]\n",
      "\n",
      "By ensuring the OPEN drawer action occurs before the subsequent PICK UP, we conform to the specifications.\n",
      "PLAN!!!!!!!!!!!!!!!!!!!!!!\n",
      "MultiVLM Response: I observed that the task's step [OPEN, drawer] causes the drawer to obstruct the tennis ball, which results in the robot failing to pick up the tennis ball.\n",
      "\n",
      "Correct task:\n",
      "1. [PICK UP, apple]\n",
      "2. [PUT INTO, holder]\n",
      "3. [PICK UP, tennis] (In order not to be obstructed after opening the drawer)\n",
      "4. [PUT ON, desk](safe place, make sure there are no objects in hand)\n",
      "5. [OPEN, drawer] (Then open the drawer open)\n",
      "6. [PICK UP, tennis]\n",
      "7. [PUT INTO, drawer]\n",
      "8. [CLOSE, drawer]\n",
      "FORMAT!!!!!!!!!!!!!!!!!!!!!!\n",
      "Yes!!! The final generated plan meets the specifications. \n",
      "After the first action picking up the apple, the second action can be [PUT INTO, holder]. \n",
      "After the second action [PUT INTO, holder], the third action can be [PICK UP, tennis]. \n",
      "After the third action picking up the tennis, the fourth action can be [PUT ON, desk]. \n",
      "After the fourth action [PUT ON, desk], the fifth action can be [OPEN, drawer]. \n",
      "After the fifth action [OPEN, drawer], the sixth action can be [PICK UP, tennis]. \n",
      "After the sixth action picking up the tennis, the seventh action can be [PUT INTO, drawer]. \n",
      "After the seventh action [PUT INTO, drawer], the eighth action can be [CLOSE, drawer].\n"
     ]
    }
   ],
   "source": [
    "task = \"Put the apple and the tennis ball in either the drawer or the pen holder, together or separately. Ensure the drawer is closed.\"\n",
    "task_plan = \"1. [PICK UP, apple]2. [PUT INTO, holder]3. [OPEN, drawer]4. [PICK UP, tennis]5. [PUT INTO, drawer]6. [CLOSE, drawer]\"\n",
    "subtask = \"[PICK UP, tennis]\"\n",
    "env_objects = \"apple, drawer, tennis, holder, desk\"\n",
    "object_origin_state = \"[ontop, tennis, desk]\"\n",
    "object_target_state = \"[inhand, tennis]\"\n",
    "image_url = ['./examples/images/vlm_frames/frame_7.jpg', './examples/images/vlm_frames/frame_8.jpg', './examples/images/vlm_frames/frame_9.jpg']\n",
    "\n",
    "reflective_plan(task=task, task_plan=task_plan, subtask=subtask, env_objects=env_objects, object_origin_state=object_origin_state, object_target_state=object_target_state, image_url=image_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simworld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
